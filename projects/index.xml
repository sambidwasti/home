<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Sambid Wasti</title><link>https://sambidwasti.com/projects/</link><description>Recent content in Projects on Sambid Wasti</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 25 Jun 2022 18:35:46 +0530</lastBuildDate><atom:link href="https://sambidwasti.com/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>Kaggle Titanic</title><link>https://sambidwasti.com/projects/kag-titanic/</link><pubDate>Sat, 25 Jun 2022 18:35:46 +0530</pubDate><guid>https://sambidwasti.com/projects/kag-titanic/</guid><description>Overview: This is the kaggle competition used to learn machine learning. In this competition, we are given a database with information about the passengers on board. We have a test set that has information about if the passengers survived or not. Then we have a different dataset where we do not know if they survived or not. We have to determine if they survived or not depending on the other information.</description></item><item><title>Principle Component Analysis (PCA)</title><link>https://sambidwasti.com/projects/pca/</link><pubDate>Sat, 25 Jun 2022 18:35:46 +0530</pubDate><guid>https://sambidwasti.com/projects/pca/</guid><description>Overview: I noticed randomm articles that try to explain PCA and use python packages that has PCA to look at data. Most of the time, it is refrenced as dimension reduction but it does not address the beauty behind the mathematics of this process. I have attempted to document a simple example here, which i have also explained in a section in my Phd. dessertation.
Principle Component Analysis (PCA) PCA is an powerful tool that uses eigen vectors and eigen values to find the correlated variables and collapse them to reduce the dimensions.</description></item></channel></rss>