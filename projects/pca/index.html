<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=icon href=https://sambidwasti.github.io/home/profile_pic1_lowres.png type=image/gif><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=https://sambidwasti.github.io/home/css/font.css media=all><meta property="og:title" content="Principle Component Analysis (PCA)"><meta property="og:description" content="Quick example of PCA"><meta property="og:type" content="article"><meta property="og:url" content="https://sambidwasti.github.io/home/projects/pca/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2022-06-25T18:35:46+05:30"><meta property="article:modified_time" content="2022-06-25T18:35:46+05:30"><meta property="og:site_name" content="Sambid Wasti"><meta name=twitter:card content="summary"><meta name=twitter:title content="Principle Component Analysis (PCA)"><meta name=twitter:description content="Quick example of PCA"><link rel=stylesheet href=https://sambidwasti.github.io/home/bootstrap-5/css/bootstrap.min.css media=all><link rel=stylesheet href=https://sambidwasti.github.io/home/css/header.css media=all><link rel=stylesheet href=https://sambidwasti.github.io/home/css/footer.css media=all><link rel=stylesheet href=https://sambidwasti.github.io/home/css/theme.css media=all><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--background-color:#eaedf0;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--link-color:#007bff;--link-color-glow:#3c00ff;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--background-color-dark:#18191a;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529;--link-color-dark:#00b2f8;--link-color-glow-dark:#0eeffb}body{font-size:1rem;font-weight:400;line-height:1.5;text-align:left}</style><meta name=description content="Principle Component Analysis (PCA) of Sambid Wasti"><title>Principle Component Analysis (PCA) | Sambid Wasti</title></head><body class=light onload=loading()><header><nav class="pt-3 navbar navbar-expand-lg"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand secondary-font text-wrap" href=https://sambidwasti.github.io/home/><img src=https://sambidwasti.github.io/home/profile_pic1_lowres.png width=30 height=30 class="d-inline-block align-top"><div class="d-inline-block px-2">Sambid Wasti</div></a><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation"><svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap secondary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text"><a class=nav-link href=https://sambidwasti.github.io/home/ aria-label=home>Home</a></li><li class="nav-item navbar-text"><a class=nav-link href=https://sambidwasti.github.io/home/exp/main_experience title="Experience Main">Experience</a></li><li class="nav-item navbar-text"><a class=nav-link href=https://sambidwasti.github.io/home/projects title=Projects>Projects</a></li></ul></div></div></nav></header><div id=content><div class="container pt-5" id=list-page><h2 class="text-center pb-2">Principle Component Analysis (PCA)</h2><h6 class=text-center>Quick example of PCA</h6><div class="row p-4"><h1 id=overview>Overview:</h1><p>I noticed randomm articles that try to explain PCA and use python packages that has PCA to look at data. Most of the time, it is refrenced as dimension reduction but it does not address the beauty behind the mathematics of this process. I have attempted to document a simple example here, which i have also explained in a section in my <a href=https://arxiv.org/abs/2012.12939>Phd. dessertation</a>.</p><h1 id=principle-component-analysis-pca>Principle Component Analysis (PCA)</h1><p>PCA is an powerful tool that uses eigen vectors and eigen values to find the correlated variables and collapse them to reduce the dimensions. This is better explained by various steps.</p><h2 id=1-initial-data>1. Initial data.</h2><p>Let us assume a dataset, that is randomly peturbed along y=x and multipled by a constant which showcases the power of PCA in correlated dataset. And lets assume that we have a z variable that depends on some combination of x and y.</p><table><thead><tr><th>x</th><th>y</th><th>z</th></tr></thead><tbody><tr><td>2.50</td><td>24.0</td><td>0.40</td></tr><tr><td>0.50</td><td>7.00</td><td>0.69</td></tr><tr><td>2.20</td><td>29.00</td><td>1.92</td></tr><tr><td>1.90</td><td>22.00</td><td>0.75</td></tr><tr><td>3.10</td><td>30.00</td><td>7.48</td></tr><tr><td>2.30</td><td>27.00</td><td>4.84</td></tr><tr><td>2.00</td><td>16.00</td><td>0.06</td></tr><tr><td>1.00</td><td>11.00</td><td>9.20</td></tr><tr><td>1.50</td><td>16.00</td><td>5.28</td></tr><tr><td>1.10</td><td>9.00</td><td>8.79</td></tr></tbody></table><p><img src="https://drive.google.com/uc?id=1wGKFbk-d_Z1UqXTD1EcxLOlV4JUM-ML9" alt="sample image" style="display:block;margin-right:auto;margin-left:auto;width:90%;box-shadow:0 4px 8px rgba(0,0,0,.2),0 6px 20px rgba(0,0,0,.19)"></p><h2 id=2-standardize-the-data>2. Standardize the data.</h2><p>This is an important step to normalize the data. Another way to think about it is that, this removes the units from the data as they are normalized or centered at 0(or its mean) and amount of std from 0. This is done by standardizing the data which includes substracting the mean and dividing by the standard deviation.</p><table><thead><tr><th>Sx</th><th>Sy</th></tr></thead><tbody><tr><td>0.9262</td><td>0.6101</td></tr><tr><td>-1.7585</td><td>-1.5067</td></tr><tr><td>0.5235</td><td>1.2328</td></tr><tr><td>0.1208</td><td>0.3611</td></tr><tr><td>1.7317</td><td>1.3573</td></tr><tr><td>0.6577</td><td>0.9837</td></tr><tr><td>0.2551</td><td>-0.3861</td></tr><tr><td>-1.0874</td><td>-1.0086</td></tr><tr><td>-0.4162</td><td>-3.8602</td></tr><tr><td>-0.9531</td><td>-1.2577</td></tr></tbody></table><p>As we can see below, we only scaled the axis but the feature of the data is the same.</p><p>using the sklearn, we can use python to achieve this.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> StandardScaler
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack((x, y)) <span style=color:#75715e># Assuming we have X and Y np.array</span>
</span></span><span style=display:flex><span>scaler <span style=color:#f92672>=</span> StandardScaler()
</span></span><span style=display:flex><span>data_scaled <span style=color:#f92672>=</span> scaler<span style=color:#f92672>.</span>fit_transform(data)
</span></span></code></pre></div><p>This results in data_scaled as standardized variable for the original data.</p><p><img src="https://drive.google.com/uc?id=1xWi7SCIYK-fWBQeJ8W8hGT3UKhVOzLKQ" alt="sample image" style="display:block;margin-right:auto;margin-left:auto;width:90%;box-shadow:0 4px 8px rgba(0,0,0,.2),0 6px 20px rgba(0,0,0,.19)"></p><h2 id=correlationcovariance-matrix>Correlation/Covariance Matrix.</h2><p>We can then get the covariance/correlation matrix.
This is basically, a 2x2 matrix where we have xx,xy,yx,yy correlations. The xx value is determined by multiplying, ith x component with ith x component, and then summing all of them and dividing it by n (no. of i elements). This should be 1 for xx and yy column. The xy or yx is done by multiplying ith x with ith y and summing them. The correlation or standardized covariance should be same but looks like there is a differnce in formula where the division for covariance is done by n-1 instead of n. If the dataset is large, the difference between n and n-1 is very little.</p><p>The correlation matrix thus comes out to be:</p><pre tabindex=0><code>  1.0 0.926
0.926 1.0
</code></pre><p>The 0.926 value means that x and y are highly correlated. The positive sign means that its positively correalatd.</p><h2 id=eigen-values-and-eigen-vectors>Eigen values and Eigen vectors.</h2><p>The eigen values and eigen vectors helps to understand the variation in the data.
We can get the eigen values and vectors via</p><pre tabindex=0><code>eigenvalues, eigenvectors = np.linalg.eig(cor_mat)
</code></pre><p>which results in eigen values as: 1.926, 0.074 and the corresponding eigen vectors are:</p><pre tabindex=0><code>[0.70710678, -0.70710678] 
[0.70710678,  0.70710678]
</code></pre><p>respectively. First column is the eigen vector for 1st value. (0.707, 0.707) represents the 1.926 eigen value.</p><p>The eigen values and vectors define the data:</p><ul><li>The eigen values define the variation present in the data.</li><li>Here we see that the first eigen values covers 1.926 amount and second 0.074.</li><li>The values itself doesnt make much sense so we can use a % value out of the total 100% (1.926 + 0.074) of 2.</li><li>So the first eigen vector (1.926) covers 96.2% of the data and the second only 3.8%.</li><li>The eigen vectors associated with them define the data and covers the variation accordingly.</li><li>Each eigen vectors are orthogonal to each other.</li></ul><p>Visually, what it means is shown below where blue line is the eigen vector1 and green is the eigen vector 2. (Note: Usually this is defined by an arrow.)</p><p><img src="https://drive.google.com/uc?id=13kODws-xuDRtWZA_uIjzBiX5jwnYYb1L" alt="sample image" style="display:block;margin-right:auto;margin-left:auto;width:90%;box-shadow:0 4px 8px rgba(0,0,0,.2),0 6px 20px rgba(0,0,0,.19)"></p><h2 id=principle-components>Principle Components.</h2><p>In the above picture, the co-ordinate axis is defined by SX and SY. However, we can represent it by the axis along eigen vectors. But the eigen vectors are just a vector, we need to know the transformed co-ordinates (new variables) that takes it from (SX,SY) to (eig1,eig2). The Principle Components are the new variables that are along the eigenvalue coordinates.</p><p>This rotation is done simply by:<br>PC1_i = SX_i * Eig1[0] + SY_i * Eig1[1]<br>PC2_i = SX_i * Eig2[0] + SY_i * Eig2[1]</p><table><thead><tr><th>PC1</th><th>PC2</th></tr></thead><tbody><tr><td>1.0864</td><td>-0.2235</td></tr><tr><td>-2.3089</td><td>0.1781</td></tr><tr><td>1.2419</td><td>0.5015</td></tr><tr><td>0.3407</td><td>0.1699</td></tr><tr><td>2.1843</td><td>-0.2647</td></tr><tr><td>1.1607</td><td>0.2305</td></tr><tr><td>-0.0926</td><td>-0.4533</td></tr><tr><td>-1.4821</td><td>0.0557</td></tr><tr><td>-0.5672</td><td>0.02130</td></tr><tr><td>-1.5633</td><td>-0.2153</td></tr></tbody></table><blockquote><p>Note: We get exactly the same no. of PCs as the beginning variables. We had 2 starting variables, x and y so we have 2 PCs.</p></blockquote><p>Now lets see the data in the PC1 and PC2 space which is the same data just rotated.</p><p><img src="https://drive.google.com/uc?id=1iVRJtg9LI76TRfHaosIpMVBXqNgq1GDM" alt="sample image" style="display:block;margin-right:auto;margin-left:auto;width:90%;box-shadow:0 4px 8px rgba(0,0,0,.2),0 6px 20px rgba(0,0,0,.19)"></p><h2 id=collapsing-the-dimension>Collapsing the dimension.</h2><p>The most important concept of the PCA is to reducing the no. of dependent variables. This can be thought of also as removing the noise.</p><p>Here, since most of the variation is defined by the 1st eigen value, and the first eigen vectors and hence the 1st PC, we can just represent the data with just one variable PC1 and collapse the PC2 dimension and we have the data defined by 1 variable PC1 instead of 2 variables. One thing to note is that, PC1 still consists of x and y, but is a linear combination of those values.</p><p>The resulting plot will be as follows.</p><p><img src="https://drive.google.com/uc?id=1ZL-RXqFmERa1gxxpbid7yz00azMNXRoY" alt="sample image" style="display:block;margin-right:auto;margin-left:auto;width:90%;box-shadow:0 4px 8px rgba(0,0,0,.2),0 6px 20px rgba(0,0,0,.19)"></p><p>By doing the above, we are only loosing less than 4% of the variation in the data.</p><h2 id=discussion-on-no-of-variables>Discussion on no. of variables.</h2><p>Usually we have a plethora of variables and applying this method reduces the no. of variables by a lot. The least variation covering PCs could be considered noise or not affecting the data. If there are 2 variables that are depended on each other (or correlated), PCA approach collapses that to 1, and make the other obsolelete, like the example above.</p><p>One of the most important discussion should be about how many variables or PC should be used. This is the discussion that is rarely seen in the tutorials and projects. The cumulative variation covered by the no. of PCs should be the motivating factor to determine the number of PCs. And the no. of variation covered should be determined by the project. It should not be directly assuming we want certain no. of variables.</p><p>For example if there were 15 variables and 15 PCs. If 6 of them covered 90% and 8 of them covered 95% and 10 of them covered 98%, so if the project requires or defines 95%, then 8 of them should be used.</p><h2 id=the-aftermath-of-pc>The aftermath of PC</h2><p>After getting the PCs, we have reduced the number of variables but the data has to be fitted. In our original problem, we had z as a function of x and y. Now we are just fitting z as a function of PC1. This way the problem is reduced from 2 variables to 1. For a large datasets, the PC approach reduces the no. of variables by half or more. Therefore it is an important tool.</p></div><div class="card-group p-4"><div class="row justify-content-center"></div></div></div></div><footer><div class="text-center pt-2"><span style=... class=px-2><a href=https://www.linkedin.com/in/sambidwasti/ target=_blank class=social-icon><i class="fab fa-3x fa-linkedin-in" style=color:#0072b1></i></a></span>
<span style=... class=px-2><a href=https://github.com/sambidwasti/ target=_blank class=social-icon><i class="fab fa-3x fa-github" style=color:green></i></a></span>
<span style=... class=px-2><a href=https://gitlab.com/sambidwasti target=_blank class=social-icon><i class="fab fa-3x fa-gitlab" style=color:#ffc300></i></a></span>
<span style=... class=px-2><a href="https://scholar.google.com/citations?hl=en&amp;user=0x9HPbMAAAAJ" target=_blank class=social-icon><i class="fa fa-3x fa-graduation-cap" style=color:#4c8bf5></i></a></span></div><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center">&copy; 2023 All Rights Reserved<div class=text-secondary>Made with
<span class=text-danger>&#10084;</span>
and
<a href=https://gohugo.io/ target=_blank title=Hugo>Hugo</a></div></div></div></div></footer><script src=https://sambidwasti.github.io/home/bootstrap-5/js/bootstrap.bundle.min.js></script>
<script>document.body.className.includes("light")&&(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))</script><script>let loadingIcons;function loading(){myVar=setTimeout(showPage,100)}function showPage(){try{document.getElementById("loading-icons").style.display="block"}catch{}}</script></body></html>